import asyncio
import aiohttp
import aiofiles
import os
import re
import json
import ssl
import xml.etree.ElementTree as ET
from datetime import datetime
import redis.asyncio as aioredis  # ä½¿ç”¨å¼‚æ­¥ç‰ˆ Redis å®¢æˆ·ç«¯
import requests  # æ·»åŠ åŒæ­¥è¯·æ±‚åº“ä½œä¸ºå¤‡ç”¨

# é…ç½®å‚æ•°
OUTPUT_DIR = "wapo_articles"  # è¾“å‡ºç›®å½•
LOG_FILE = os.path.join(OUTPUT_DIR, "crawler_log.txt")  # æ—¥å¿—æ–‡ä»¶
ERROR_LOG_FILE = os.path.join(OUTPUT_DIR, "error_log.txt")  # é”™è¯¯æ—¥å¿—
BATCH_SIZE = 5  # å¹¶å‘çˆ¬å–æ•°é‡
REQUEST_DELAY = 0.5  # è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
CATEGORIES = ["world", "science", "business", "us", "politics", "national"]

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)

async def log_message(message, file_path=LOG_FILE):
    """è®°å½•æ¶ˆæ¯åˆ°æ—¥å¿—æ–‡ä»¶å¹¶è¾“å‡ºåˆ°æ§åˆ¶å°"""
    async with aiofiles.open(file_path, "a", encoding="utf-8") as f:
        await f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\n")
    print(message)

async def fetch_sitemap(url, session):
    """è·å–ç«™ç‚¹åœ°å›¾å†…å®¹"""
    # æ·»åŠ æµè§ˆå™¨æ ·å¼çš„User-Agent
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Pragma': 'no-cache',
        'Cache-Control': 'no-cache'
    }

    try:
        # ç¦ç”¨SSLéªŒè¯ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # æ—¥å¿—è®°å½•è¯·æ±‚å°è¯•
        await log_message(f"å°è¯•è·å–ç«™ç‚¹åœ°å›¾: {url}")
        
        async with session.get(url, headers=headers, ssl=ssl_context, timeout=30) as response:
            await log_message(f"è·å–ç«™ç‚¹åœ°å›¾çŠ¶æ€ç : {response.status}")
            if response.status == 200:
                content = await response.text()
                # æ£€æŸ¥å†…å®¹é•¿åº¦
                content_length = len(content)
                await log_message(f"æˆåŠŸè·å–ç«™ç‚¹åœ°å›¾ï¼Œå†…å®¹é•¿åº¦: {content_length} å­—èŠ‚")
                return content
            else:
                await log_message(f"è·å–ç«™ç‚¹åœ°å›¾å¤±è´¥: {url} (çŠ¶æ€ç : {response.status})", ERROR_LOG_FILE)
                return None
    except asyncio.TimeoutError:
        await log_message(f"è·å–ç«™ç‚¹åœ°å›¾è¶…æ—¶: {url}", ERROR_LOG_FILE)
        return None
    except Exception as e:
        await log_message(f"è·å–ç«™ç‚¹åœ°å›¾é”™è¯¯: {url} ({str(e)})", ERROR_LOG_FILE)
        import traceback
        await log_message(traceback.format_exc(), ERROR_LOG_FILE)
        return None

def get_wapo_sitemap_url():
    """æ ¹æ®å½“å‰å¹´æœˆè·å–åç››é¡¿é‚®æŠ¥ç«™ç‚¹åœ°å›¾URL"""
    current_date = datetime.today()
    year = current_date.strftime("%Y")
    month = current_date.strftime("%m")
    
    # æ„å»ºåç››é¡¿é‚®æŠ¥çš„ç«™ç‚¹åœ°å›¾URL
    sitemap_url = f"https://www.washingtonpost.com/sitemaps/sitemap-{year}-{month}.xml"
    
    return sitemap_url

def fetch_sitemap_sync(url):
    """åŒæ­¥æ–¹å¼è·å–ç«™ç‚¹åœ°å›¾ï¼ˆä½œä¸ºå¤‡ç”¨æ–¹æ³•ï¼‰"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }
    
    try:
        # ç¦ç”¨SSLéªŒè¯
        response = requests.get(url, headers=headers, verify=False, timeout=30)
        if response.status_code == 200:
            return response.text
        return None
    except Exception:
        return None

async def get_wapo_article_urls():
    """è·å–åç››é¡¿é‚®æŠ¥ä»Šæ—¥æ–‡ç« çš„URLåˆ—è¡¨"""
    today = datetime.today()
    today_str = today.strftime("%Y-%m-%d")
    
    await log_message(f"ğŸ” å¼€å§‹çˆ¬å–åç››é¡¿é‚®æŠ¥ä»Šæ—¥ ({today_str}) æ–‡ç« ...")
    
    # æ ¹æ®å½“å‰å¹´æœˆè·å–ç«™ç‚¹åœ°å›¾URL
    sitemap_url = get_wapo_sitemap_url()
    await log_message(f"ğŸ“„ ä½¿ç”¨ç«™ç‚¹åœ°å›¾: {sitemap_url}")
    
    # ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ç¡¬ç¼–ç çš„URL (ç”¨äºæµ‹è¯•)
    # sitemap_url = "https://www.washingtonpost.com/sitemaps/sitemap-2024-03.xml"
    # await log_message(f"ğŸ“„ ä½¿ç”¨æµ‹è¯•ç«™ç‚¹åœ°å›¾: {sitemap_url}")
    
    # é…ç½® ClientSessionï¼Œç¦ç”¨SSLéªŒè¯
    conn = aiohttp.TCPConnector(ssl=False, force_close=True)
    async with aiohttp.ClientSession(connector=conn, timeout=aiohttp.ClientTimeout(total=60)) as session:
        try:
            # å°è¯•å¼‚æ­¥è·å–
            sitemap_content = await fetch_sitemap(sitemap_url, session)
            
            # å¦‚æœå¼‚æ­¥è·å–å¤±è´¥ï¼Œå°è¯•åŒæ­¥è·å–
            if not sitemap_content:
                await log_message("å°è¯•ä½¿ç”¨åŒæ­¥æ–¹å¼è·å–ç«™ç‚¹åœ°å›¾...")
                sitemap_content = fetch_sitemap_sync(sitemap_url)
                
                if sitemap_content:
                    await log_message("âœ… åŒæ­¥æ–¹å¼è·å–ç«™ç‚¹åœ°å›¾æˆåŠŸ")
                
            # å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°è¯•è·å–ä¸Šä¸ªæœˆçš„ç«™ç‚¹åœ°å›¾
            if not sitemap_content:
                await log_message(f"âŒ æ— æ³•è·å–ç«™ç‚¹åœ°å›¾: {sitemap_url}ï¼Œå°è¯•è·å–ä¸Šä¸ªæœˆçš„ç«™ç‚¹åœ°å›¾")
                
                # å¦‚æœå½“å‰æœˆä»½çš„ç«™ç‚¹åœ°å›¾ä¸å­˜åœ¨ï¼Œå°è¯•è·å–ä¸Šä¸ªæœˆçš„
                last_month = datetime.today().replace(day=1)
                if last_month.month == 1:
                    last_month = last_month.replace(year=last_month.year-1, month=12)
                else:
                    last_month = last_month.replace(month=last_month.month-1)
                    
                year = last_month.strftime("%Y")
                month = last_month.strftime("%m")
                sitemap_url = f"https://www.washingtonpost.com/sitemaps/sitemap-{year}-{month}.xml"
                
                await log_message(f"ğŸ“„ å°è¯•ä½¿ç”¨ä¸Šä¸ªæœˆç«™ç‚¹åœ°å›¾: {sitemap_url}")
                sitemap_content = await fetch_sitemap(sitemap_url, session)
                
                # å¦‚æœå¼‚æ­¥è·å–å¤±è´¥ï¼Œä¹Ÿå°è¯•åŒæ­¥è·å–
                if not sitemap_content:
                    await log_message("å°è¯•ä½¿ç”¨åŒæ­¥æ–¹å¼è·å–ä¸Šä¸ªæœˆç«™ç‚¹åœ°å›¾...")
                    sitemap_content = fetch_sitemap_sync(sitemap_url)
                    
                    if sitemap_content:
                        await log_message("âœ… åŒæ­¥æ–¹å¼è·å–ä¸Šä¸ªæœˆç«™ç‚¹åœ°å›¾æˆåŠŸ")
                
                # å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨2024å¹´çš„ç«™ç‚¹åœ°å›¾
                if not sitemap_content:
                    # å°è¯•2024å¹´çš„å½“å‰æœˆä»½
                    test_sitemap_url = f"https://www.washingtonpost.com/sitemaps/sitemap-2024-{month}.xml"
                    await log_message(f"ğŸ“„ å°è¯•ä½¿ç”¨2024å¹´ç«™ç‚¹åœ°å›¾: {test_sitemap_url}")
                    sitemap_content = await fetch_sitemap(test_sitemap_url, session)
                    
                    if not sitemap_content:
                        sitemap_content = fetch_sitemap_sync(test_sitemap_url)
                
                # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥
                if not sitemap_content:
                    await log_message(f"âŒ æ— æ³•è·å–ä»»ä½•ç«™ç‚¹åœ°å›¾ï¼Œç¨‹åºç»“æŸ")
                    return []
            
            await log_message(f"âœ… æˆåŠŸè·å–ç«™ç‚¹åœ°å›¾: {sitemap_url}")
            
            # å°è¯•è§£æXML
            try:
                # å¯¹äºç®€å•çš„XMLå¤„ç†ï¼Œç›´æ¥ä½¿ç”¨namespace
                namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
                root = ET.fromstring(sitemap_content)
                
                all_articles = []
                url_count = 0
                
                # éå†æ‰€æœ‰URLå…ƒç´ å¹¶è®¡æ•°
                for url_elem in root.findall(".//url", None) or root.findall(".//ns:url", namespace):
                    url_count += 1
                
                await log_message(f"ç«™ç‚¹åœ°å›¾ä¸­å…±æœ‰ {url_count} ä¸ªURL")
                
                # ç”±äºWPçš„sitemapç»“æ„ç®€å•ï¼Œç›´æ¥æŒ‰ç…§ç¤ºä¾‹è¿›è¡Œè§£æ
                for url_elem in root.findall(".//url", None) or root.findall(".//ns:url", namespace):
                    try:
                        # å°è¯•ä¸¤ç§æ–¹å¼è·å–locå…ƒç´ 
                        loc_elem = url_elem.find("loc") or url_elem.find("ns:loc", namespace)
                        if loc_elem is None:
                            continue
                        
                        article_url = loc_elem.text
                        if not article_url:
                            continue
                            
                        # æå–ç±»åˆ«å’Œæ ‡é¢˜ï¼ˆåŸºäºURLï¼‰
                        url_parts = article_url.split('/')
                        
                        # è·³è¿‡éæ–‡ç« URL
                        skip_patterns = ["/index.html", "/newsletters/", "/video/", "/live-updates/"]
                        if any(pattern in article_url for pattern in skip_patterns):
                            continue
                        
                        # æå–ç±»åˆ«
                        category = None
                        if len(url_parts) >= 4:
                            possible_category = url_parts[3].lower()
                            # æ£€æŸ¥URLç¬¬ä¸€æ®µæ˜¯å¦ä¸ºç±»åˆ«
                            if possible_category in CATEGORIES:
                                category = possible_category
                            # ç‰¹æ®Šå¤„ç†ä¸€äº›URL
                            elif "politics" in article_url:
                                category = "politics"
                            elif "national" in article_url:
                                category = "national"
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç±»åˆ«ï¼Œä½¿ç”¨"other"
                            else:
                                category = "other"
                        
                        # å°è¯•è·å–lastmodå…ƒç´ 
                        lastmod_elem = url_elem.find("lastmod") or url_elem.find("ns:lastmod", namespace)
                        pub_date = lastmod_elem.text if lastmod_elem is not None else None
                        
                        # æ„å»ºæ–‡ç« å¯¹è±¡
                        article = {
                            "website": "Washington Post",
                            "url": article_url,
                            "title": os.path.basename(article_url.rstrip('/')).replace('-', ' ').title(),  # ä»URLç”Ÿæˆä¸´æ—¶æ ‡é¢˜
                            "date": pub_date or "æœªçŸ¥æ—¥æœŸ",
                            "image": None,
                            "category": category
                        }
                        
                        all_articles.append(article)
                        
                    except Exception as e:
                        await log_message(f"å¤„ç†æ–‡ç« é”™è¯¯: {str(e)}", ERROR_LOG_FILE)
                
                await log_message(f"âœ… ä»ç«™ç‚¹åœ°å›¾ä¸­æå–äº† {len(all_articles)} ç¯‡æ–‡ç« ")
                
                # é™åˆ¶æ–‡ç« æ•°é‡
                if len(all_articles) > 20:
                    all_articles = all_articles[:20]
                    await log_message(f"âš ï¸ é™åˆ¶æ–‡ç« æ•°é‡ä¸ºå‰20ç¯‡")
                
                return all_articles
                
            except ET.ParseError as e:
                await log_message(f"è§£æç«™ç‚¹åœ°å›¾é”™è¯¯: {str(e)}", ERROR_LOG_FILE)
                
                # å°è¯•æ‰‹åŠ¨è§£æç®€å•çš„XML
                await log_message("å°è¯•æ‰‹åŠ¨è§£æXML...")
                
                all_articles = []
                # ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§£æ
                url_matches = re.findall(r'<loc>(.*?)</loc>', sitemap_content)
                date_matches = re.findall(r'<lastmod>(.*?)</lastmod>', sitemap_content)
                
                # è®°å½•æ‰¾åˆ°çš„URLæ•°é‡
                await log_message(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ° {len(url_matches)} ä¸ªURLå’Œ {len(date_matches)} ä¸ªæ—¥æœŸ")
                
                # å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•URLï¼Œå°è¯•è®°å½•åŸå§‹å†…å®¹
                if not url_matches and len(sitemap_content) < 1000:
                    await log_message(f"ç«™ç‚¹åœ°å›¾å†…å®¹è¿‡çŸ­æˆ–æ ¼å¼ä¸ç¬¦ï¼ŒåŸå§‹å†…å®¹: {sitemap_content}", ERROR_LOG_FILE)
                
                for i, url in enumerate(url_matches):
                    try:
                        date = date_matches[i] if i < len(date_matches) else "æœªçŸ¥æ—¥æœŸ"
                        
                        # è·³è¿‡éæ–‡ç« URL
                        skip_patterns = ["/index.html", "/homepage/", "/newsletters/", "/arcpublishing/"]
                        if any(pattern in url for pattern in skip_patterns):
                            continue
                        
                        # æå–ç±»åˆ«ï¼ˆåŸºäºURLï¼‰
                        category = None
                        for cat in CATEGORIES:
                            if f"/{cat}/" in url.lower():
                                category = cat
                                break
                        
                        # å¦‚æœæ²¡æ‰¾åˆ°ç±»åˆ«ï¼Œæ£€æŸ¥æ›´å¤šå¯èƒ½çš„ç±»åˆ«
                        if not category:
                            if "/sports/" in url.lower():
                                category = "sports"
                            elif "/technology/" in url.lower():
                                category = "technology"
                            elif "/health/" in url.lower():
                                category = "health"
                            elif "/education/" in url.lower():
                                category = "education"
                            elif "/weather/" in url.lower():
                                category = "weather"
                            else:
                                category = "other"
                        
                        # ä»URLæå–æ ‡é¢˜
                        slug = os.path.basename(url.rstrip('/'))
                        title = slug.replace('-', ' ').title()
                        
                        article = {
                            "website": "Washington Post",
                            "url": url,
                            "title": title,
                            "date": date,
                            "image": None,
                            "category": category
                        }
                        
                        all_articles.append(article)
                    except Exception as e:
                        await log_message(f"å¤„ç†æ–‡ç« é”™è¯¯: {str(e)}", ERROR_LOG_FILE)
                
                await log_message(f"âœ… é€šè¿‡æ‰‹åŠ¨è§£ææå–äº† {len(all_articles)} ç¯‡æ–‡ç« ")
                
                # é™åˆ¶æ–‡ç« æ•°é‡
                if len(all_articles) > 20:
                    all_articles = all_articles[:20]
                    await log_message(f"âš ï¸ é™åˆ¶æ–‡ç« æ•°é‡ä¸ºå‰20ç¯‡")
                
                return all_articles
                
        except Exception as e:
            await log_message(f"å¤„ç†ç«™ç‚¹åœ°å›¾é”™è¯¯: {str(e)}", ERROR_LOG_FILE)
            import traceback
            await log_message(traceback.format_exc(), ERROR_LOG_FILE)
            return []

async def fetch_article_details(article, session):
    """ä»æ–‡ç« ç½‘é¡µè·å–æ›´å¤šè¯¦æƒ…"""
    url = article["url"]
    
    # å¦‚æœå·²ç»æœ‰æ ‡é¢˜å°±ä¸å†è·å–è¯¦æƒ…
    # ç”±äºWPç½‘ç«™åŠ è½½é€Ÿåº¦è¾ƒæ…¢ï¼Œè¿™é‡Œåªè·å–åŸºæœ¬ä¿¡æ¯ï¼Œä¸å†è·å–è¯¦æƒ…
    return article
    
    # ä»¥ä¸‹ä»£ç æš‚æ—¶æ³¨é‡Šæ‰ï¼Œé¿å…è¯·æ±‚ç½‘é¡µè¯¦æƒ…å¯¼è‡´ç¨‹åºè¶…æ—¶
    # å¦‚æœéœ€è¦è·å–è¯¦æƒ…ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Š
    """
    try:
        # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
            if response.status != 200:
                return article
            
            html_content = await response.text()
            
            # è·å–æ ‡é¢˜
            title_match = re.search(r'<meta property="og:title" content="([^"]+)"', html_content)
            if title_match:
                article["title"] = title_match.group(1).strip()
            else:
                title_match = re.search(r'<title>(.*?)</title>', html_content)
                if title_match:
                    title = title_match.group(1).replace(" - The Washington Post", "").strip()
                    article["title"] = title
            
            # è·å–å›¾ç‰‡
            img_match = re.search(r'<meta property="og:image" content="([^"]+)"', html_content)
            if img_match:
                article["image"] = img_match.group(1)
            
        return article
    except asyncio.TimeoutError:
        await log_message(f"è·å–æ–‡ç« è¯¦æƒ…è¶…æ—¶: {url}", ERROR_LOG_FILE)
        return article
    except Exception as e:
        await log_message(f"è·å–æ–‡ç« è¯¦æƒ…é”™è¯¯: {url} ({str(e)})", ERROR_LOG_FILE)
        return article
    """

async def save_articles(articles):
    """ä¿å­˜æ–‡ç« ä¿¡æ¯åˆ°æ–‡ä»¶"""
    json_file = os.path.join(OUTPUT_DIR, "articles.json")
    
    try:
        async with aiofiles.open(json_file, "w", encoding="utf-8") as f:
            await f.write(json.dumps(articles, ensure_ascii=False, indent=4))
        await log_message(f"âœ… æ–‡ç« æ•°æ®å·²ä¿å­˜åˆ° {json_file}")
    except Exception as e:
        await log_message(f"âŒ ä¿å­˜æ–‡ç« åˆ° JSON æ–‡ä»¶å¤±è´¥: {str(e)}", ERROR_LOG_FILE)

async def main():
    # æ¸…ç©ºæ—¥å¿—
    async with aiofiles.open(LOG_FILE, "w") as f:
        await f.write("")
    async with aiofiles.open(ERROR_LOG_FILE, "w") as f:
        await f.write("")
    
    await log_message("ğŸš€ å¼€å§‹çˆ¬å–åç››é¡¿é‚®æŠ¥æ–‡ç« ...")
    
    try:
        # è®¾ç½®æ€»ä½“è¶…æ—¶æ—¶é—´ - 30ç§’
        articles_task = asyncio.create_task(get_wapo_article_urls())
        articles = await asyncio.wait_for(articles_task, timeout=30)
        
        if not articles:
            await log_message("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œç¨‹åºç»“æŸ")
            return
        
        # ç”±äºå·²ç»ä»URLç”Ÿæˆäº†åŸºæœ¬æ ‡é¢˜ï¼Œè¿™é‡Œä¸å†è·å–è¯¦æƒ…
        await log_message(f"âœ… è·³è¿‡è·å–æ–‡ç« è¯¦æƒ…æ­¥éª¤ï¼Œç›´æ¥ä¿å­˜ {len(articles)} ç¯‡æ–‡ç« ")
        
        await save_articles(articles)
        
        # å°†çˆ¬å–åˆ°çš„æ–‡ç« æ•°æ®å­˜å…¥ Redisï¼Œé”®åï¼šwapo_articlesï¼Œç¼“å­˜æ—¶é—´ä¸º1å°æ—¶ï¼ˆ3600ç§’ï¼‰
        try:
            redis_client = await aioredis.from_url("redis://localhost:6379", encoding="utf-8", decode_responses=True)
            await redis_client.set("wapo_articles", json.dumps(articles), ex=3600)
            await log_message("âœ… æ–‡ç« æ•°æ®å·²ä¿å­˜åˆ° Redis é”® 'wapo_articles'")
        except Exception as e:
            await log_message(f"âŒ ä¿å­˜æ–‡ç« åˆ° Redis å¤±è´¥: {str(e)}", ERROR_LOG_FILE)
        
        await log_message(f"âœ… çˆ¬å–å®Œæˆ! å…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
        
    except asyncio.TimeoutError:
        await log_message("âŒ çˆ¬å–è¿‡ç¨‹è¶…æ—¶ï¼Œç¨‹åºç»“æŸ")
    except Exception as e:
        await log_message(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", ERROR_LOG_FILE)
        import traceback
        await log_message(traceback.format_exc(), ERROR_LOG_FILE)

if __name__ == "__main__":
    try:
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    except AttributeError:
        pass
    asyncio.run(main())