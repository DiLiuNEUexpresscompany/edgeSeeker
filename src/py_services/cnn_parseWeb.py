import asyncio
import aiohttp
import aiofiles
import os
import re
import json
import xml.etree.ElementTree as ET
from datetime import datetime
import redis.asyncio as aioredis  # ä½¿ç”¨å¼‚æ­¥ç‰ˆ Redis å®¢æˆ·ç«¯

# é…ç½®å‚æ•°
OUTPUT_DIR = "cnn_articles"  # è¾“å‡ºç›®å½•
LOG_FILE = os.path.join(OUTPUT_DIR, "crawler_log.txt")  # æ—¥å¿—æ–‡ä»¶
ERROR_LOG_FILE = os.path.join(OUTPUT_DIR, "error_log.txt")  # é”™è¯¯æ—¥å¿—
BATCH_SIZE = 5  # å¹¶å‘çˆ¬å–æ•°é‡
REQUEST_DELAY = 0.5  # è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
CATEGORIES = ["world", "science", "business", "us", "politics"]

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)

async def log_message(message, file_path=LOG_FILE):
    """è®°å½•æ¶ˆæ¯åˆ°æ—¥å¿—æ–‡ä»¶å¹¶è¾“å‡ºåˆ°æ§åˆ¶å°"""
    async with aiofiles.open(file_path, "a", encoding="utf-8") as f:
        await f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\n")
    print(message)

async def fetch_sitemap(url, session):
    """è·å–ç«™ç‚¹åœ°å›¾å†…å®¹"""
    try:
        async with session.get(url) as response:
            if response.status == 200:
                return await response.text()
            else:
                await log_message(f"è·å–ç«™ç‚¹åœ°å›¾å¤±è´¥: {url} (çŠ¶æ€ç : {response.status})", ERROR_LOG_FILE)
                return None
    except Exception as e:
        await log_message(f"è·å–ç«™ç‚¹åœ°å›¾é”™è¯¯: {url} ({str(e)})", ERROR_LOG_FILE)
        return None

async def get_cnn_article_urls():
    """è·å–CNNä»Šæ—¥æ–‡ç« çš„URLåˆ—è¡¨"""
    today = datetime.today()
    today_str = today.strftime("%Y-%m-%d")
    year_str = today.strftime("%Y")
    month_str = today.strftime("%m")
    
    await log_message(f"ğŸ” å¼€å§‹çˆ¬å–CNNä»Šæ—¥ ({today_str}) æ–‡ç« ...")
    
    base_url = "https://www.cnn.com/sitemap/article.xml"
    
    async with aiohttp.ClientSession() as session:
        sitemap_content = await fetch_sitemap(base_url, session)
        if not sitemap_content:
            return []
        
        await log_message(f"âœ… æˆåŠŸè·å–ä¸»ç«™ç‚¹åœ°å›¾")
        
        try:
            namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            root = ET.fromstring(sitemap_content)
            
            article_sitemaps = []
            current_month_pattern = f"/{year_str}/{month_str}"
            
            for sitemap in root.findall(".//ns:sitemap", namespace):
                loc_elem = sitemap.find("ns:loc", namespace)
                if loc_elem is not None:
                    sitemap_url = loc_elem.text
                    for category in CATEGORIES:
                        if category in sitemap_url and current_month_pattern in sitemap_url:
                            article_sitemaps.append((sitemap_url, category))
                            break
            
            await log_message(f"âœ… æ‰¾åˆ° {len(article_sitemaps)} ä¸ªå½“æœˆæ–‡ç« ç«™ç‚¹åœ°å›¾")
            
            if not article_sitemaps:
                for sitemap in root.findall(".//ns:sitemap", namespace):
                    loc_elem = sitemap.find("ns:loc", namespace)
                    if loc_elem is not None:
                        sitemap_url = loc_elem.text
                        for category in CATEGORIES:
                            if category in sitemap_url and f"/{year_str}/" in sitemap_url:
                                article_sitemaps.append((sitemap_url, category))
                                break
                await log_message(f"âœ… æ‰¾åˆ° {len(article_sitemaps)} ä¸ªå½“å¹´æ–‡ç« ç«™ç‚¹åœ°å›¾")
            
            article_sitemaps = article_sitemaps[:5]  # æœ€å¤šå¤„ç†5ä¸ªç«™ç‚¹åœ°å›¾
            all_articles = []
            
            for sitemap_url, category in article_sitemaps:
                await log_message(f"ğŸ” å¤„ç†ç«™ç‚¹åœ°å›¾: {sitemap_url} (ç±»åˆ«: {category})")
                sub_sitemap_content = await fetch_sitemap(sitemap_url, session)
                if not sub_sitemap_content:
                    continue
                
                try:
                    sub_root = ET.fromstring(sub_sitemap_content)
                    sub_namespace = {
                        'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9',
                        'image': 'http://www.google.com/schemas/sitemap-image/1.1',
                        'news': 'http://www.google.com/schemas/sitemap-news/0.9'
                    }
                    
                    today_articles = []
                    for url_elem in sub_root.findall(".//ns:url", sub_namespace):
                        try:
                            loc_elem = url_elem.find("ns:loc", sub_namespace)
                            if loc_elem is None:
                                continue
                            article_url = loc_elem.text
                            
                            lastmod_elem = url_elem.find("ns:lastmod", sub_namespace)
                            pub_date = lastmod_elem.text if lastmod_elem is not None else None
                            
                            if pub_date is None:
                                news_elem = url_elem.find(".//news:news", sub_namespace)
                                if news_elem is not None:
                                    pub_date_elem = news_elem.find(".//news:publication_date", sub_namespace)
                                    if pub_date_elem is not None:
                                        pub_date = pub_date_elem.text
                            
                            if not pub_date or today_str not in pub_date:
                                continue
                            
                            title = None
                            news_elem = url_elem.find(".//news:news", sub_namespace)
                            if news_elem is not None:
                                title_elem = news_elem.find(".//news:title", sub_namespace)
                                if title_elem is not None:
                                    title = title_elem.text
                            
                            image_url = None
                            image_elem = url_elem.find(".//image:image", sub_namespace)
                            if image_elem is not None:
                                image_loc = image_elem.find(".//image:loc", sub_namespace)
                                if image_loc is not None:
                                    image_url = image_loc.text
                                if title is None:
                                    image_title = image_elem.find(".//image:title", sub_namespace)
                                    if image_title is not None:
                                        title = image_title.text
                            
                            today_articles.append({
                                "website": "CNN",
                                "url": article_url,
                                "title": title or "æœªæ‰¾åˆ°æ ‡é¢˜",
                                "date": pub_date,
                                "image": image_url,
                                "category": category  # æ·»åŠ ç±»åˆ«æ ‡ç­¾
                            })
                        except Exception as e:
                            await log_message(f"å¤„ç†æ–‡ç« é”™è¯¯: {str(e)}", ERROR_LOG_FILE)
                    
                    await log_message(f"âœ… åœ¨ç«™ç‚¹åœ°å›¾ {sitemap_url} ä¸­æ‰¾åˆ° {len(today_articles)} ç¯‡ä»Šæ—¥æ–‡ç« ")
                    all_articles.extend(today_articles)
                    
                except ET.ParseError as e:
                    await log_message(f"è§£æç«™ç‚¹åœ°å›¾é”™è¯¯: {sitemap_url} ({str(e)})", ERROR_LOG_FILE)
            
            if len(all_articles) < 5:
                await log_message(f"âš ï¸ åªæ‰¾åˆ° {len(all_articles)} ç¯‡ä»Šæ—¥æ–‡ç« ï¼Œå°è¯•è·å–æ›´å¤šæ–‡ç« ...")
                
                for sitemap_url, category in article_sitemaps:
                    sub_sitemap_content = await fetch_sitemap(sitemap_url, session)
                    if not sub_sitemap_content:
                        continue
                    
                    try:
                        sub_root = ET.fromstring(sub_sitemap_content)
                        sub_namespace = {
                            'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9',
                            'image': 'http://www.google.com/schemas/sitemap-image/1.1',
                            'news': 'http://www.google.com/schemas/sitemap-news/0.9'
                        }
                        
                        for url_elem in sub_root.findall(".//ns:url", sub_namespace)[:10]:
                            try:
                                loc_elem = url_elem.find("ns:loc", sub_namespace)
                                if loc_elem is None:
                                    continue
                                article_url = loc_elem.text
                                
                                if any(a["url"] == article_url for a in all_articles):
                                    continue
                                
                                lastmod_elem = url_elem.find("ns:lastmod", sub_namespace)
                                pub_date = lastmod_elem.text if lastmod_elem is not None else None
                                
                                if pub_date is None:
                                    news_elem = url_elem.find(".//news:news", sub_namespace)
                                    if news_elem is not None:
                                        pub_date_elem = news_elem.find(".//news:publication_date", sub_namespace)
                                        if pub_date_elem is not None:
                                            pub_date = pub_date_elem.text
                                
                                title = None
                                news_elem = url_elem.find(".//news:news", sub_namespace)
                                if news_elem is not None:
                                    title_elem = news_elem.find(".//news:title", sub_namespace)
                                    if title_elem is not None:
                                        title = title_elem.text
                                
                                image_url = None
                                image_elem = url_elem.find(".//image:image", sub_namespace)
                                if image_elem is not None:
                                    image_loc = image_elem.find(".//image:loc", sub_namespace)
                                    if image_loc is not None:
                                        image_url = image_loc.text
                                    if title is None:
                                        image_title = image_elem.find(".//image:title", sub_namespace)
                                        if image_title is not None:
                                            title = image_title.text
                                
                                all_articles.append({
                                    "website": "CNN",
                                    "url": article_url,
                                    "title": title or "æœªæ‰¾åˆ°æ ‡é¢˜",
                                    "date": pub_date or "æœªçŸ¥æ—¥æœŸ",
                                    "image": image_url,
                                    "category": category  # æ·»åŠ ç±»åˆ«æ ‡ç­¾
                                })
                                
                                if len(all_articles) >= 20:
                                    break
                                    
                            except Exception as e:
                                await log_message(f"å¤„ç†æ–‡ç« é”™è¯¯: {str(e)}", ERROR_LOG_FILE)
                            
                    except ET.ParseError as e:
                        await log_message(f"è§£æç«™ç‚¹åœ°å›¾é”™è¯¯: {sitemap_url} ({str(e)})", ERROR_LOG_FILE)
            
            await log_message(f"âœ… æœ€ç»ˆè·å–åˆ° {len(all_articles)} ç¯‡æ–‡ç« ")
            return all_articles
            
        except ET.ParseError as e:
            await log_message(f"è§£æä¸»ç«™ç‚¹åœ°å›¾é”™è¯¯: {str(e)}", ERROR_LOG_FILE)
            return []
        except Exception as e:
            await log_message(f"å¤„ç†ç«™ç‚¹åœ°å›¾é”™è¯¯: {str(e)}", ERROR_LOG_FILE)
            import traceback
            await log_message(traceback.format_exc(), ERROR_LOG_FILE)
            return []

async def fetch_article_details(article, session):
    """ä»æ–‡ç« ç½‘é¡µè·å–æ›´å¤šè¯¦æƒ…"""
    url = article["url"]
    
    try:
        async with session.get(url) as response:
            if response.status != 200:
                return article
            
            html_content = await response.text()
            
            if article["title"] == "æœªæ‰¾åˆ°æ ‡é¢˜":
                title_match = re.search(r'<title>(.*?)</title>', html_content)
                if title_match:
                    article["title"] = title_match.group(1).replace(" - CNN", "").strip()
            
            if not article.get("image"):
                img_match = re.search(r'<meta property="og:image" content="([^"]+)"', html_content)
                if img_match:
                    article["image"] = img_match.group(1)
                else:
                    img_match = re.search(r'<img[^>]+src="([^"]+)"', html_content)
                    if img_match:
                        image_url = img_match.group(1)
                        if image_url.startswith('/'):
                            image_url = 'https://www.cnn.com' + image_url
                        article["image"] = image_url
            
        return article
    except Exception as e:
        await log_message(f"è·å–æ–‡ç« è¯¦æƒ…é”™è¯¯: {url} ({str(e)})", ERROR_LOG_FILE)
        return article

async def save_articles(articles):
    """ä¿å­˜æ–‡ç« ä¿¡æ¯åˆ°æ–‡ä»¶"""
    json_file = os.path.join(OUTPUT_DIR, "articles.json")
    
    try:
        async with aiofiles.open(json_file, "w", encoding="utf-8") as f:
            await f.write(json.dumps(articles, ensure_ascii=False, indent=4))
        await log_message(f"âœ… æ–‡ç« æ•°æ®å·²ä¿å­˜åˆ° {json_file}")
    except Exception as e:
        await log_message(f"âŒ ä¿å­˜æ–‡ç« åˆ° JSON æ–‡ä»¶å¤±è´¥: {str(e)}", ERROR_LOG_FILE)

async def main():
    # æ¸…ç©ºæ—¥å¿—
    async with aiofiles.open(LOG_FILE, "w") as f:
        await f.write("")
    async with aiofiles.open(ERROR_LOG_FILE, "w") as f:
        await f.write("")
    
    await log_message("ğŸš€ å¼€å§‹çˆ¬å–CNNæ–‡ç« ...")
    articles = await get_cnn_article_urls()
    
    if not articles:
        await log_message("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œç¨‹åºç»“æŸ")
        return
    
    await log_message(f"ğŸ” æ­£åœ¨è·å– {len(articles)} ç¯‡æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯...")
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_article_details(article, session) for article in articles]
        articles = await asyncio.gather(*tasks)
    
    await save_articles(articles)
    
    # å°†çˆ¬å–åˆ°çš„æ–‡ç« æ•°æ®å­˜å…¥ Redisï¼Œé”®åï¼šcnn_articlesï¼Œç¼“å­˜æ—¶é—´ä¸º1å°æ—¶ï¼ˆ3600ç§’ï¼‰
    try:
        redis_client = await aioredis.from_url("redis://localhost:6379", encoding="utf-8", decode_responses=True)
        await redis_client.set("cnn_articles", json.dumps(articles), ex=3600)
        await log_message("âœ… æ–‡ç« æ•°æ®å·²ä¿å­˜åˆ° Redis é”® 'cnn_articles'")
    except Exception as e:
        await log_message(f"âŒ ä¿å­˜æ–‡ç« åˆ° Redis å¤±è´¥: {str(e)}", ERROR_LOG_FILE)
    
    await log_message(f"âœ… çˆ¬å–å®Œæˆ! å…±è·å– {len(articles)} ç¯‡æ–‡ç« ")

if __name__ == "__main__":
    try:
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    except AttributeError:
        pass
    asyncio.run(main())
